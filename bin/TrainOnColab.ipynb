{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRGnpLVl46ie",
        "outputId": "3646c4e0-8ac7-4cab-bd7e-ec45a661b92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'proteoxystis'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 285 (delta 111), reused 91 (delta 55), pack-reused 137\u001b[K\n",
            "Receiving objects: 100% (285/285), 44.20 MiB | 3.41 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n",
            "/content/proteoxystis/bin\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdb (from -r requirements.txt (line 1))\n",
            "  Downloading pypdb-2.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.65.0)\n",
            "Collecting nose2 (from -r requirements.txt (line 4))\n",
            "  Downloading nose2-0.13.0-py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.6/205.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pypdb->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (3.4)\n",
            "Installing collected packages: nose2, pypdb\n",
            "Successfully installed nose2-0.13.0 pypdb-2.2\n",
            "Reading toml file\n",
            "Removing stupid data\n",
            " 99% 71814/72490 [00:00<00:00, 89546.94it/s]\n",
            "after removing stupid data, this many remain  71814\n",
            "Input array size (pdb x one-hot sequence):  (71814, 86688)\n",
            "Filling sequence array\n",
            "100% 71814/71814 [00:11<00:00, 6332.77it/s]\n",
            "Normalizing output values\n",
            "100% 377/377 [00:17<00:00, 20.96it/s]\n",
            "Output array size (pdb x one-hot output):  (71814, 754)\n",
            "Filling output array\n",
            "100% 71814/71814 [00:18<00:00, 3833.52it/s]\n",
            "Reading test data from test.toml\n",
            "Separating test data from training data\n",
            "100% 71814/71814 [00:04<00:00, 15263.83it/s]\n",
            "Test data size:  (17953, 86688)\n",
            "Training data size:  (53861, 86688)\n",
            "Pickeling data\n"
          ]
        }
      ],
      "source": [
        "# Get an A100 high memory instance Premium +\n",
        "# Get repo, change to bin directory, \n",
        "!git lfs install\n",
        "!git clone https://github.com/retospect/proteoxystis.git\n",
        "%cd proteoxystis/bin\n",
        "# install dependencies\n",
        "!pip install -r requirements.txt\n",
        "# Prep data\n",
        "!python3 ./pickelize.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask for more memory\n",
        "#a = [\"yo\"]\n",
        "#while(1):\n",
        "#    a.append(\"\".join(a))"
      ],
      "metadata": {
        "id": "PhIQxoFZ8zGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "# should be in bin directory\n",
        "#!ls\n",
        "#%cd proteoxystis/bin\n",
        "# Train on the data\n",
        "!python3 ./train.py --new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsfIZub07OiM",
        "outputId": "f64ba6d9-79d2-4e92-c049-69ecb991eff0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/proteoxystis/bin\n",
            "Loading data...done\n",
            "seqs.shape:    (53861, 86688)\n",
            "output.shape:  (53861, 754)\n",
            "Setting seeds to 42 ...done\n",
            "Using CUDA\n",
            "Setting up new model (will overwrite the old if it exists)...ael:  21\n",
            "D:  53861\n",
            "C:  754\n",
            "done\n",
            "Sending data to device...done\n",
            "Running model first time...Traceback (most recent call last):\n",
            "  File \"/content/proteoxystis/bin/./train.py\", line 484, in <module>\n",
            "    main()\n",
            "  File \"/content/proteoxystis/bin/./train.py\", line 450, in main\n",
            "    train(model, seqs, output, args.epochs, relevant_train)\n",
            "  File \"/content/proteoxystis/bin/./train.py\", line 226, in train\n",
            "    output_pred = model(in_data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 313, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 309, in _conv_forward\n",
            "    return F.conv1d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.88 GiB (GPU 0; 39.56 GiB total capacity; 20.54 GiB already allocated; 18.06 GiB free; 20.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    }
  ]
}