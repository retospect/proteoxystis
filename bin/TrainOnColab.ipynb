{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRGnpLVl46ie",
        "outputId": "45c69713-22bc-4368-cae7-0c831ec2099d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'proteoxystis'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 336 (delta 152), reused 115 (delta 72), pack-reused 137\u001b[K\n",
            "Receiving objects: 100% (336/336), 44.56 MiB | 18.21 MiB/s, done.\n",
            "Resolving deltas: 100% (233/233), done.\n",
            "Filtering content: 100% (3/3), 54.17 MiB | 22.68 MiB/s, done.\n",
            "/root\n",
            "/root/proteoxystis/bin\n",
            "Branch '16bitfloat' set up to track remote branch '16bitfloat' from 'origin'.\n",
            "Switched to a new branch '16bitfloat'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdb (from -r requirements.txt (line 1))\n",
            "  Downloading pypdb-2.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.65.0)\n",
            "Collecting nose2 (from -r requirements.txt (line 4))\n",
            "  Downloading nose2-0.13.0-py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.6/205.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pypdb->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pypdb->-r requirements.txt (line 1)) (3.4)\n",
            "Installing collected packages: nose2, pypdb\n",
            "Successfully installed nose2-0.13.0 pypdb-2.2\n",
            "Reading toml file\n",
            "Removing stupid data\n",
            " 99% 70287/70896 [00:00<00:00, 90336.47it/s]\n",
            "after removing stupid data, this many remain  70287\n",
            "Input array size (pdb x one-hot sequence):  (70287, 4128, 21)\n",
            "Filling sequence array\n",
            "100% 70287/70287 [00:11<00:00, 6316.57it/s]\n",
            "Normalizing output values\n",
            "  5% 18/349 [00:00<00:15, 21.95it/s]/root/proteoxystis/bin/./pickelize.py:114: RuntimeWarning: overflow encountered in true_divide\n",
            "  values = values / factor\n",
            "/root/proteoxystis/bin/./pickelize.py:116: RuntimeWarning: invalid value encountered in subtract\n",
            "  values = values - offset\n",
            "100% 349/349 [00:16<00:00, 21.26it/s]\n",
            "Output array size (pdb x one-hot output):  (70287, 698)\n",
            "Filling output array\n",
            "100% 70287/70287 [00:15<00:00, 4446.55it/s]\n",
            "Reading test data from test.toml\n",
            "Separating test data from training data\n",
            "100% 70287/70287 [00:09<00:00, 7609.94it/s]\n",
            "Test data size:  (17571, 4128, 21)\n",
            "Training data size:  (52716, 4128, 21)\n",
            "Pickeling data\n"
          ]
        }
      ],
      "source": [
        "# Get an A100 high memory instance Premium +\n",
        "# Get repo, change to bin directory, \n",
        "!git lfs install\n",
        "!git clone https://github.com/retospect/proteoxystis.git\n",
        "%cd ~\n",
        "%cd proteoxystis/bin\n",
        "!git checkout 16bitfloat\n",
        "# install dependencies\n",
        "!pip install -r requirements.txt\n",
        "# Prep data\n",
        "!python3 ./pickelize.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From class example:\n"
      ],
      "metadata": {
        "id": "bTf4Z-9jHu4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data:\n",
        "import pickle\n",
        "\n",
        "print(\"Loading data...\", end=\"\", flush=True)\n",
        "# gzip the pickle is possible but way slow\n",
        "with open(\"training_data.pickle\", \"rb\") as f:\n",
        "    (\n",
        "        metadata,\n",
        "        seqs,\n",
        "        output,\n",
        "        seqs_test,\n",
        "        output_test,\n",
        "        relevant_train,\n",
        "        relevant_test,\n",
        "    ) = pickle.load(f)\n",
        "print(\"done\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FEwUYyVMULj",
        "outputId": "50f60138-c7fd-4c6a-eafd-173028fb0002"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set the random seed for reproducible results\n",
        "torch.manual_seed(1)\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # This just calls the base class constructor\n",
        "        super().__init__()\n",
        "        # Neural network layers assigned as attributes of a Module subclass\n",
        "        # have their parameters registered for training automatically.\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
        "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The RNN also returns its hidden state but we don't use it.\n",
        "        # While the RNN can also take a hidden state as input, the RNN\n",
        "        # gets passed a hidden state initialized with zeros by default.\n",
        "        h = self.rnn(x)[0]\n",
        "        x = self.linear(h)\n",
        "        return x\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=7,batch_first=True,dropout=0.3)\n",
        "        self.linear = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear1 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.drop = torch.nn.Dropout(0.25)\n",
        "        self.drop0 = torch.nn.Dropout(0.25)\n",
        "        self.softmax1 = torch.nn.Softmax()\n",
        "        self.softmax2 = torch.nn.Softmax()\n",
        "        self.linear4 = torch.nn.Linear(hidden_size, output_size)\n",
        "        print(\"Hidden size\", hidden_size)\n",
        "        print(\"output size\", output_size)\n",
        "        print(\"input size\", input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.lstm(x)[0]\n",
        "        x = self.linear(h)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "    \n",
        "    def get_states_across_time(self, x):\n",
        "        h_c = None\n",
        "        h_list, c_list = list(), list()\n",
        "        with torch.no_grad():\n",
        "            for t in range(x.size(1)):\n",
        "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
        "                h_list.append(h_c[0])\n",
        "                c_list.append(h_c[1])\n",
        "            h = torch.cat(h_list)\n",
        "            c = torch.cat(c_list)\n",
        "        return h, c"
      ],
      "metadata": {
        "id": "5QD6hiMfJuaV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data_gen, criterion, optimizer, device):\n",
        "    # Set the model to training mode. This will turn on layers that would\n",
        "    # otherwise behave differently during evaluation, such as dropout.\n",
        "    model.train()\n",
        "\n",
        "    # Store the number of sequences that were classified correctly\n",
        "    num_correct = 0\n",
        "\n",
        "    # Iterate over every batch of sequences. Note that the length of a data generator\n",
        "    # is defined as the number of batches required to produce a total of roughly 1000\n",
        "    # sequences given a batch size.\n",
        "    for batch_idx in range(len(train_data_gen)):\n",
        "        if batch_idx%25 ==0:\n",
        "          print(\",\", end=\"\", flush=True)\n",
        "        # Request a batch of sequences and class labels, convert them into tensors\n",
        "        # of the correct type, and then send them to the appropriate device.\n",
        "        data, target, mask = train_data_gen[batch_idx]\n",
        "        data, target, mask = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device), torch.from_numpy(mask).float().to(device)\n",
        "\n",
        "        # Perform the forward pass of the model\n",
        "        output = model(data)  # Step ①\n",
        "\n",
        "        # Pick only the output corresponding to last sequence element (input is pre padded)\n",
        "        output = output[:, -1, :]\n",
        "        #output = mask*output#\n",
        "        # Compute the value of the loss for this batch. For loss functions like CrossEntropyLoss,\n",
        "        # the second argument is actually expected to be a tensor of class indices rather than\n",
        "        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n",
        "        # of the target and call argmax along its second dimension to create a tensor of shape\n",
        "        # (batch_size) containing the index of the class label that was hot for each sequence.\n",
        "        target = target.argmax(dim=1)\n",
        "        #print(output.shape, target.shape)\n",
        "        loss = criterion(output, target)  # Step ②\n",
        "\n",
        "        # Clear the gradient buffers of the optimized parameters.\n",
        "        # Otherwise, gradients from the previous batch would be accumulated.\n",
        "        optimizer.zero_grad()  # Step ③\n",
        "\n",
        "        loss.backward()  # Step ④\n",
        "\n",
        "        optimizer.step()  # Step ⑤\n",
        "\n",
        "        y_pred = output.argmax(dim=1)\n",
        "        num_correct += (y_pred == target).sum().item()\n",
        "    print(\":\", end=\"\", flush=True)\n",
        "    return num_correct, loss.item()"
      ],
      "metadata": {
        "id": "qo-b5EJEJxnc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_data_gen, criterion, device):\n",
        "    # Set the model to evaluation mode. This will turn off layers that would\n",
        "    # otherwise behave differently during training, such as dropout.\n",
        "    model.eval()\n",
        "\n",
        "    # Store the number of sequences that were classified correctly\n",
        "    num_correct = 0\n",
        "\n",
        "    # A context manager is used to disable gradient calculations during inference\n",
        "    # to reduce memory usage, as we typically don't need the gradients at this point.\n",
        "    with torch.no_grad():\n",
        "        for batch_idx in range(len(test_data_gen)):\n",
        "            data, target, mask = test_data_gen[batch_idx]\n",
        "            data, target, mask = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device), torch.from_numpy(mask).float().to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            # Pick only the output corresponding to last sequence element (input is pre padded)\n",
        "            output = output[:, -1, :]\n",
        "            #output =mask*output\n",
        "            target = target.argmax(dim=1)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            y_pred = output.argmax(dim=1)\n",
        "            num_correct += (y_pred == target).sum().item()\n",
        "\n",
        "    return num_correct, loss.item()"
      ],
      "metadata": {
        "id": "cdqZbCJtJ_fp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#from res.plot_lib import set_default, plot_state, print_colourbar\n",
        "#set_default()"
      ],
      "metadata": {
        "id": "gMZztt_yKHRU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True):\n",
        "    # Automatically determine the device that PyTorch should use for computation\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Move model to the device which will be used for train and test\n",
        "    model.to(device)\n",
        "\n",
        "    # Track the value of the loss function and model accuracy across epochs\n",
        "    history_train = {'loss': [], 'acc': []}\n",
        "    history_test = {'loss': [], 'acc': []}\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Run the training loop and calculate the accuracy.\n",
        "        # Remember that the length of a data generator is the number of batches,\n",
        "        # so we multiply it by the batch size to recover the total number of sequences.\n",
        "        num_correct, loss = train(model, train_data_gen, criterion, optimizer, device)\n",
        "        accuracy = float(num_correct) / (len(train_data_gen) )* len(train_data_gen) * 100 #fixme\n",
        "        history_train['loss'].append(loss)\n",
        "        history_train['acc'].append(accuracy)\n",
        "\n",
        "        # Do the same for the testing loop\n",
        "        num_correct, loss = test(model, test_data_gen, criterion, device)\n",
        "        accuracy = float(num_correct) / (len(test_data_gen) ) * len(test_data_gen) * 100 #FIXME\n",
        "        history_test['loss'].append(loss)\n",
        "        history_test['acc'].append(accuracy)\n",
        "\n",
        "        print(f'\\n[Epoch {epoch + 1}/{max_epochs}]'\n",
        "                  f\" loss: {history_train['loss'][-1]:.4f}, acc: {history_train['acc'][-1]:2.2f}%\"\n",
        "                  f\" - test_loss: {history_test['loss'][-1]:.4f}, test_acc: {history_test['acc'][-1]:2.2f}%\")\n",
        "\n",
        "    # Generate diagnostic plots for the loss and accuracy\n",
        "    fig, axes = plt.subplots(ncols=2, figsize=(9, 4.5))\n",
        "    for ax, metric in zip(axes, ['loss', 'acc']):\n",
        "        ax.plot(history_train[metric])\n",
        "        ax.plot(history_test[metric])\n",
        "        ax.set_xlabel('epoch', fontsize=12)\n",
        "        ax.set_ylabel(metric, fontsize=12)\n",
        "        ax.legend(['Train', 'Test'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "VhGOCTWTKaiv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Setup the training and test data generators\n",
        "        # seqs,\n",
        "        # output,\n",
        "        # seqs_test,\n",
        "        # output_test,\n",
        "        # relevant_test,\n",
        "        # relevant_train,\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "setlen = 256\n",
        "print(\"len\",len(seqs))\n",
        "numpieces = len(seqs)//setlen\n",
        "longbit = numpieces*setlen\n",
        "print(\"pcs\",numpieces)\n",
        "print(len(seqs))\n",
        "print(longbit)\n",
        "seqs2,_ = np.split(seqs, [longbit])\n",
        "output2,_ = np.split(output, [longbit])\n",
        "relevant_train2,_ = np.split(relevant_train, [longbit])\n",
        "seqs2 = np.split(seqs2,setlen)\n",
        "output2 = np.split(output2, setlen)\n",
        "relevant_train2 = np.split(relevant_train2, setlen)\n",
        "\n",
        "print(len(output2))\n",
        "trainy = []\n",
        "for i in range(setlen):\n",
        "  trainy.append((seqs2[i], output2[i], relevant_train2[i]))\n",
        "\n",
        "## Test\n",
        "\n",
        "setlen = 256\n",
        "print(len(seqs_test))\n",
        "numpieces = len(seqs_test)//setlen\n",
        "longbit = numpieces*setlen\n",
        "print(numpieces)\n",
        "print(len(seqs_test))\n",
        "print(longbit)\n",
        "seqs2,_ = np.split(seqs_test, [longbit])\n",
        "output2,_ = np.split(output_test, [longbit])\n",
        "relevant_train2,_ = np.split(relevant_train, [longbit])\n",
        "seqs2 = np.split(seqs2,setlen)\n",
        "output2 = np.split(output2, setlen)\n",
        "relevant_test2,_ = np.split(relevant_train2, [longbit])\n",
        "print(len(output2))\n",
        "testy = []\n",
        "for i in range(setlen):\n",
        "  testy.append((seqs2[i], output2[i], relevant_test2[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHeSMbP0OVq-",
        "outputId": "a1317a20-2ca0-46a3-cdee-080517aecd3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len 52716\n",
            "pcs 205\n",
            "52716\n",
            "52480\n",
            "256\n",
            "17571\n",
            "68\n",
            "17571\n",
            "17408\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 4128\n",
        "# Setup the RNN and training settings\n",
        "input_size  = 21 \n",
        "hidden_size = 16\n",
        "output_size = 698\n",
        "model       = SimpleLSTM(input_size, hidden_size, output_size)\n",
        "criterion   = torch.nn.CrossEntropyLoss()#torch.nn.MSELoss()#\n",
        "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "max_epochs  = 10\n",
        "\n",
        "# Train the model\n",
        "model = train_and_test(model, trainy, testy, criterion, optimizer, max_epochs, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKW0H1bgMAgq",
        "outputId": "72239222-0c44-4fbc-a04a-9ddaf6d337f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden size 16\n",
            "output size 698\n",
            "input size 21\n",
            ",,,"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDB4CkGFShQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}